{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import hashlib\n",
    "import tempfile\n",
    "import bittensor as bt\n",
    "from types import SimpleNamespace\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import LlamaForCausalLM, LlamaConfig, LlamaTokenizer\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import boto3\n",
    "import torch\n",
    "import wandb\n",
    "import typer\n",
    "import argparse\n",
    "import tempfile\n",
    "import bittensor as bt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from dotenv import dotenv_values\n",
    "from types import SimpleNamespace\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Instantiate my S3 client.\n",
    "env_config = {**dotenv_values(\".env\"), **os.environ}\n",
    "AWS_ACCESS_KEY_ID = env_config.get('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = env_config.get('AWS_SECRET_ACCESS_KEY')\n",
    "client: boto3.client = boto3.client(\n",
    "    's3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key = AWS_SECRET_ACCESS_KEY\n",
    ")\n",
    "bucket = 'decis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model-5D2uD6jYfLvpMPsngA3n4VAqxxmXSPtuku1TaXQnPdH3xohZ.pt', 'model-5D2uD6jYfLvpMPsngA3n4VAqxxmXSPtuku1TaXQnPdH3xohZ_metadata.json', 'model-5DHYeSMbG3n31Ev9e9BNZzBiWnSd9bkp5s8CdC3RR5aMUpAh.pt', 'model-5DHYeSMbG3n31Ev9e9BNZzBiWnSd9bkp5s8CdC3RR5aMUpAh_metadata.json', 'model-5EkVGe9zdK4D1rdPD2qutAbU1HGNcjSt8ibsMQcWnPLNcsxn.pt', 'model-5EkVGe9zdK4D1rdPD2qutAbU1HGNcjSt8ibsMQcWnPLNcsxn_metadata.json', 'model-5F4UUMWF41GsLFvwVpigmoxTKVngDP6C7utvECgtQB83U3fJ.pt', 'model-5F4UUMWF41GsLFvwVpigmoxTKVngDP6C7utvECgtQB83U3fJ_metadata.json', 'model-5GQw7vvU1tzQ8XCVE1FRf3As9MmTZasPZUoiSgDmnnEj9jaP.pt', 'model-5GQw7vvU1tzQ8XCVE1FRf3As9MmTZasPZUoiSgDmnnEj9jaP_metadata.json']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = client.list_objects_v2( Bucket = bucket )\n",
    "file_names = [content['Key'] for content in response.get('Contents', [])]\n",
    "print (file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel( config = GPT2Config(\n",
    "    output_hidden_states = False, \n",
    "    n_positions = 1024\n",
    "))\n",
    "\n",
    "wallet = bt.wallet('Alice', 'Alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest file with the largest block: gradients-5F4UUMWF41GsLFvwVpigmoxTKVngDP6C7utvECgtQB83U3fJ-11.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gradients-5F4UUMWF41GsLFvwVpigmoxTKVngDP6C7utvECgtQB83U3fJ-11.pt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_latest_metadata_file( hotkey ) -> str:\n",
    "    response = client.list_objects_v2( Bucket = bucket )\n",
    "    file_names = [content['Key'] for content in response.get('Contents', [])]\n",
    "    max_block = -1\n",
    "    latest_file = None\n",
    "    for file_name in file_names:\n",
    "        if file_name.startswith(f'gradients-{wallet.hotkey.ss58_address}-') and file_name.endswith('.pt'):\n",
    "            try:\n",
    "                block = int(file_name.split('-')[-1].split('.')[0])\n",
    "                if block > max_block:\n",
    "                    max_block = block\n",
    "                    latest_file = file_name\n",
    "            except ValueError:\n",
    "                continue\n",
    "    print(f\"Latest file with the largest block: {latest_file}\")\n",
    "    return latest_file\n",
    "\n",
    "get_latest_metadata_file( wallet.hotkey.ss58_address )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = 11\n",
    "filename = f'gradients-{wallet.hotkey.ss58_address}-{block}.pt'  # Filename for the gradients\n",
    "gradients = {name: param.data for name, param in model.named_parameters() if param.grad is not None}\n",
    "with io.BytesIO() as gradients_buffer:\n",
    "    torch.save(gradients, gradients_buffer)  # Save the gradients to the buffer\n",
    "    gradients_buffer.seek(0)  # Reset the buffer's position to the beginning\n",
    "    client.upload_fileobj(gradients_buffer, bucket, filename)  # Upload the gradients buffer to the storage service\n",
    "    client.put_object_acl(\n",
    "        Bucket=bucket,\n",
    "        Key=filename,\n",
    "        GrantRead='uri=\"http://acs.amazonaws.com/groups/global/AllUsers\"',\n",
    "        GrantReadACP='uri=\"http://acs.amazonaws.com/groups/global/AllUsers\"'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28502488, 22687085, 57214137, 41008498, 28242177, 9211837, 27743392, 14221368, 15647757, 57330970]\n"
     ]
    }
   ],
   "source": [
    "eval_pages = SubsetFineWebEdu2Loader.next_pages( offset = 10000003000, n_pages = 10, seed = 9 )\n",
    "print ([e[1] for e in eval_pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22687085, 57214137, 41008498, 28242177, 9211837, 27743392, 14221368, 15647757, 57330970, 47115201]\n"
     ]
    }
   ],
   "source": [
    "eval_pages = SubsetFineWebEdu2Loader.next_pages( offset = 10000003001, n_pages = 10, seed = 9 )\n",
    "print ([e[1] for e in eval_pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from dataset import SubsetFineWebEdu2Loader\n",
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained( 'gpt2', verbose=False, clean_up_tokenization_spaces=True )\n",
    "tokenizer.pad_token = tokenizer.eos_token    \n",
    "\n",
    "\n",
    "eval_pages = SubsetFineWebEdu2Loader.next_pages( offset = 100000, n_pages = 10, seed = 10 )\n",
    "            \n",
    "# training_pages = [ random.choice( eval_pages ) ]\n",
    "# print ('training page', training_pages )\n",
    "\n",
    "# dataset = SubsetFineWebEdu2Loader(\n",
    "#     batch_size = 10,\n",
    "#     sequence_length = 2048,\n",
    "#     pages_info = training_pages,\n",
    "#     tokenizer = tokenizer\n",
    "# )\n",
    "# print (dataset)\n",
    "\n",
    "# for batch in dataset:\n",
    "#     print (batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
